# F1-MARS Training & Evaluation Scripts

Scripts para entrenar y evaluar agentes de F1-MARS.

## üìÅ Scripts Disponibles

| Script | Descripci√≥n | Agente | Algoritmos | Documentaci√≥n |
|--------|-------------|--------|------------|---------------|
| **train_pilot.py** | Entrena el piloto aut√≥nomo (control del coche) | Pilot | PPO, SAC, TD3 | [‚Üí Gu√≠a Completa](TRAIN_PILOT.md) |
| **train_engineer.py** | Entrena el ingeniero de carrera (estrategia) | Engineer | DQN | [‚Üí Gu√≠a Completa](TRAIN_ENGINEER.md) |
| **evaluate.py** | Eval√∫a modelos entrenados con m√©tricas detalladas | Ambos | Todos | [‚Üí Gu√≠a de Evaluaci√≥n](../docs/EVALUATION_GUIDE.md) |

---

## üöÄ Quick Start

### Entrenar Piloto

```bash
# PPO (recomendado para principiantes)
python scripts/train_pilot.py --algorithm PPO --total-timesteps 500000

# Con curriculum learning autom√°tico
python scripts/train_pilot.py --curriculum --total-timesteps 1000000
```

**‚Üí [Gu√≠a completa de entrenamiento del piloto](TRAIN_PILOT.md)**

### Entrenar Ingeniero

```bash
# Estrategia con DQN
python scripts/train_engineer.py --track monza --timesteps 500000
```

**‚Üí [Gu√≠a completa de entrenamiento del ingeniero](TRAIN_ENGINEER.md)**

### Evaluar Modelo

```bash
# Evaluaci√≥n b√°sica
python scripts/evaluate.py --model trained_models/PPO_default_final.zip

# Con visualizaci√≥n y grabaci√≥n
python scripts/evaluate.py \
    --model trained_models/PPO_final.zip \
    --episodes 20 \
    --record \
    --output results/
```

**‚Üí [Gu√≠a completa de evaluaci√≥n](../docs/EVALUATION_GUIDE.md)**

---

## üéØ Casos de Uso

### 1. Quiero entrenar un piloto desde cero

```bash
# Opci√≥n A: Training tradicional (PPO)
python scripts/train_pilot.py \
    --algorithm PPO \
    --total-timesteps 500000

# Opci√≥n B: Con curriculum learning (recomendado)
python scripts/train_pilot.py \
    --curriculum \
    --total-timesteps 1000000
```

**Ver:** [TRAIN_PILOT.md](TRAIN_PILOT.md) - Secci√≥n "Algoritmos Disponibles"

### 2. Quiero comparar diferentes algoritmos

```bash
# Entrenar con cada algoritmo
python scripts/train_pilot.py --algorithm PPO --total-timesteps 500000
python scripts/train_pilot.py --algorithm SAC --total-timesteps 500000
python scripts/train_pilot.py --algorithm TD3 --total-timesteps 500000

# Comparar dos modelos
python scripts/evaluate.py \
    --model trained_models/PPO_default_final.zip \
    --compare trained_models/SAC_default_final.zip
```

**Ver:** [TRAIN_PILOT.md](TRAIN_PILOT.md) - Secci√≥n "Comparaci√≥n de Algoritmos"

### 3. Quiero entrenar en un circuito espec√≠fico

```bash
python scripts/train_pilot.py \
    --track tracks/monza.json \
    --algorithm PPO \
    --total-timesteps 500000
```

**Ver:** [TRAIN_PILOT.md](TRAIN_PILOT.md) - Secci√≥n "Training Options"

### 4. Quiero entrenar estrategia de pit stops

```bash
python scripts/train_engineer.py \
    --track monza \
    --timesteps 500000 \
    --tensorboard
```

**Ver:** [TRAIN_ENGINEER.md](TRAIN_ENGINEER.md)

### 5. Quiero evaluar y comparar todos mis modelos

```bash
# Evaluar todos los modelos en trained_models/
for model in trained_models/*.zip; do
    python scripts/evaluate.py --model "$model" --episodes 10
done

# Comparaci√≥n directa de dos modelos
python scripts/evaluate.py \
    --model trained_models/PPO_v1.zip \
    --compare trained_models/SAC_v1.zip \
    --episodes 20
```

**Ver:** Secci√≥n "Evaluaci√≥n Batch" m√°s abajo

---

## üìä Comparaci√≥n de Algoritmos

| Algoritmo | Tipo | Velocidad | Estabilidad | Exploraci√≥n | Mejor Para |
|-----------|------|-----------|-------------|-------------|------------|
| **PPO** | On-policy | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Principiantes, entrenamiento estable |
| **SAC** | Off-policy | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Circuitos complejos, convergencia r√°pida |
| **TD3** | Off-policy | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Control preciso, time trials |
| **DQN** | Off-policy | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Decisiones discretas (estrategia) |

**‚Üí Detalles completos:** [TRAIN_PILOT.md - Comparaci√≥n de Algoritmos](TRAIN_PILOT.md#comparaci√≥n-de-algoritmos)

---

## üîÑ Workflow Completo

### Pipeline End-to-End

```bash
# 1. Entrenar piloto con curriculum
python scripts/train_pilot.py \
    --curriculum \
    --algorithm PPO \
    --total-timesteps 1000000 \
    --model-dir models/pilot/

# 2. Entrenar ingeniero de estrategia
python scripts/train_engineer.py \
    --track monza \
    --timesteps 500000 \
    --model-dir models/engineer/

# 3. Evaluar piloto
python scripts/evaluate.py \
    --model models/pilot/PPO_multi_final.zip \
    --episodes 20 \
    --output results/pilot/

# 4. Evaluar ingeniero
python scripts/evaluate.py \
    --model models/engineer/engineer_final_monza.zip \
    --episodes 20 \
    --output results/engineer/

# 5. Ver resultados en TensorBoard
tensorboard --logdir logs/
```

---

## üìà Evaluaci√≥n Batch de M√∫ltiples Modelos

### Script para Evaluar Todos los Modelos

Crear archivo `evaluate_all.sh`:

```bash
#!/bin/bash

# Configuraci√≥n
EPISODES=20
OUTPUT_BASE="results/batch_eval"
TRACKS=("tracks/oval.json" "tracks/monza.json" "tracks/technical.json")

# Crear directorio de salida
mkdir -p "$OUTPUT_BASE"

# Evaluar cada modelo
for model in trained_models/*.zip; do
    model_name=$(basename "$model" .zip)
    echo "Evaluando: $model_name"

    # Evaluar en cada circuito
    for track in "${TRACKS[@]}"; do
        track_name=$(basename "$track" .json)
        output_dir="$OUTPUT_BASE/${model_name}/${track_name}"

        python scripts/evaluate.py \
            --model "$model" \
            --track "$track" \
            --episodes "$EPISODES" \
            --output "$output_dir"
    done
done

echo "Evaluaci√≥n completa! Resultados en: $OUTPUT_BASE"
```

**Ejecutar:**
```bash
chmod +x evaluate_all.sh
./evaluate_all.sh
```

### Comparar Todos los Algoritmos

```bash
#!/bin/bash

# Comparar PPO vs SAC vs TD3
TRACK="tracks/monza.json"
EPISODES=20

# Entrenar cada algoritmo
for algo in PPO SAC TD3; do
    python scripts/train_pilot.py \
        --algorithm "$algo" \
        --track "$TRACK" \
        --total-timesteps 500000 \
        --model-dir "models/${algo}/"
done

# Evaluar cada uno
for algo in PPO SAC TD3; do
    python scripts/evaluate.py \
        --model "models/${algo}/${algo}_monza_final.zip" \
        --track "$TRACK" \
        --episodes "$EPISODES" \
        --output "results/${algo}/"
done

# Comparaciones directas
python scripts/evaluate.py \
    --model "models/PPO/PPO_monza_final.zip" \
    --compare "models/SAC/SAC_monza_final.zip" \
    --episodes "$EPISODES" \
    --output "results/comparison_PPO_vs_SAC/"

python scripts/evaluate.py \
    --model "models/SAC/SAC_monza_final.zip" \
    --compare "models/TD3/TD3_monza_final.zip" \
    --episodes "$EPISODES" \
    --output "results/comparison_SAC_vs_TD3/"
```

### Generar Reporte de Comparaci√≥n

Python script `compare_all.py`:

```python
#!/usr/bin/env python3
"""
Compara todos los modelos y genera un reporte CSV.
"""

import json
import csv
from pathlib import Path
from glob import glob

results_dir = Path("results/batch_eval")
output_file = "comparison_report.csv"

# Recopilar m√©tricas de todos los modelos
all_results = []

for json_file in glob(str(results_dir / "*/*/*.json")):
    with open(json_file, 'r') as f:
        data = json.load(f)

        metrics = data['metrics']
        all_results.append({
            'model': data['model'],
            'track': data['track'],
            'completion_rate': metrics['completion_rate'],
            'lap_time_mean': metrics['lap_time_mean'],
            'lap_time_best': metrics['lap_time_best'],
            'on_track_percentage': metrics['on_track_percentage'],
            'off_track_count': metrics['off_track_count_total'],
        })

# Guardar a CSV
with open(output_file, 'w', newline='') as f:
    if all_results:
        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())
        writer.writeheader()
        writer.writerows(all_results)

print(f"Reporte guardado: {output_file}")
print(f"Total de evaluaciones: {len(all_results)}")
```

**Ejecutar:**
```bash
python compare_all.py
```

---

## üìö Documentaci√≥n Completa

### Gu√≠as de Entrenamiento

- **[TRAIN_PILOT.md](TRAIN_PILOT.md)** - Entrenamiento completo del piloto
  - Comparaci√≥n detallada PPO vs SAC vs TD3
  - Hyperpar√°metros y configuraci√≥n
  - Curriculum learning
  - Transfer learning
  - Workflows completos

- **[TRAIN_ENGINEER.md](TRAIN_ENGINEER.md)** - Entrenamiento del ingeniero
  - Estrategia de pit stops
  - Gesti√≥n de neum√°ticos
  - DQN para decisiones discretas

### Gu√≠as de Evaluaci√≥n

- **[../docs/EVALUATION_GUIDE.md](../docs/EVALUATION_GUIDE.md)** - Evaluaci√≥n detallada
  - M√©tricas completas
  - Visualizaciones
  - Comparaci√≥n de modelos
  - Grabaci√≥n de videos

### Documentaci√≥n Adicional

- **[../docs/CURRICULUM_LEARNING.md](../docs/CURRICULUM_LEARNING.md)** - Curriculum learning
- **[../tracks/README.md](../tracks/README.md)** - Creaci√≥n de circuitos
- **[../README.md](../README.md)** - Documentaci√≥n principal del proyecto

---

## üí° Tips

1. **Empieza con PPO** - Es el m√°s estable y f√°cil de usar
2. **Usa curriculum learning** - Para mejor generalizaci√≥n (`--curriculum`)
3. **Monitorea con TensorBoard** - `tensorboard --logdir logs/`
4. **Guarda checkpoints frecuentes** - Ya configurado por defecto
5. **Eval√∫a regularmente** - Usa `evaluate.py` para tracking
6. **Compara algoritmos** - Prueba PPO, SAC y TD3 para encontrar el mejor
7. **CPU es suficiente** - √ìptimo para RL con entornos paralelos

---

## ‚ö° Comandos R√°pidos

```bash
# Entrenar piloto (PPO, curriculum)
python scripts/train_pilot.py --curriculum --total-timesteps 1000000

# Entrenar piloto (SAC, circuito espec√≠fico)
python scripts/train_pilot.py --algorithm SAC --track tracks/monza.json --total-timesteps 500000

# Entrenar ingeniero
python scripts/train_engineer.py --track monza --timesteps 500000

# Evaluar modelo
python scripts/evaluate.py --model trained_models/PPO_final.zip --episodes 20

# Comparar modelos
python scripts/evaluate.py --model MODEL1.zip --compare MODEL2.zip --episodes 10

# Ver TensorBoard
tensorboard --logdir logs/
```

---

**Para m√°s detalles, consulta las gu√≠as espec√≠ficas de cada script.**
