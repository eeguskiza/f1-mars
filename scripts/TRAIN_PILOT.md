# Entrenamiento del Piloto Aut√≥nomo

Gu√≠a completa para entrenar el agente piloto que controla el coche (steering, throttle, brake).

---

## üìã √çndice

- [Algoritmos Disponibles](#algoritmos-disponibles)
- [C√≥mo Entrenar](#c√≥mo-entrenar)
- [C√≥mo Evaluar](#c√≥mo-evaluar)
- [Comparaci√≥n de Todos los Modelos](#comparaci√≥n-de-todos-los-modelos)

---

## Algoritmos Disponibles

El piloto soporta **3 algoritmos de RL** para control continuo.

### PPO (Proximal Policy Optimization)

**Recomendado para:** Principiantes, entrenamiento estable, primera vez

#### Caracter√≠sticas

| Aspecto | Valoraci√≥n | Detalles |
|---------|-----------|----------|
| **Estabilidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Muy estable, converge de forma confiable |
| **Velocidad** | ‚≠ê‚≠ê‚≠ê | Moderada, requiere m√°s pasos que SAC |
| **Facilidad de uso** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | F√°cil de configurar, pocos hiperpar√°metros |
| **Sample efficiency** | ‚≠ê‚≠ê‚≠ê | Requiere bastantes muestras (on-policy) |
| **Exploraci√≥n** | ‚≠ê‚≠ê‚≠ê | Exploraci√≥n moderada |

#### Pros
- ‚úÖ **Muy estable**: Rara vez diverge o falla
- ‚úÖ **Pocos hiperpar√°metros**: F√°cil de tunear
- ‚úÖ **Predecible**: Comportamiento consistente
- ‚úÖ **Bajo uso de memoria**: No necesita replay buffer grande
- ‚úÖ **Funciona bien out-of-the-box**: Configuraci√≥n por defecto suele ser buena

#### Cons
- ‚ö†Ô∏è **M√°s lento que SAC**: Requiere m√°s timesteps para convergencia
- ‚ö†Ô∏è **Exploraci√≥n limitada**: Puede quedarse en √≥ptimos locales
- ‚ö†Ô∏è **On-policy**: No puede reutilizar experiencias antiguas

#### Cu√°ndo Usar PPO

- ‚úì Primera vez entrenando RL
- ‚úì Necesitas resultados confiables
- ‚úì Recursos computacionales limitados
- ‚úì Circuitos sencillos a moderados
- ‚úì Prefieres estabilidad sobre velocidad

#### Ejemplo de Entrenamiento

```bash
# B√°sico (recomendado para empezar)
python scripts/train_pilot.py \
    --algorithm PPO \
    --total-timesteps 500000

# Con curriculum learning
python scripts/train_pilot.py \
    --algorithm PPO \
    --curriculum \
    --total-timesteps 1000000

# Circuito espec√≠fico
python scripts/train_pilot.py \
    --algorithm PPO \
    --track tracks/monza.json \
    --total-timesteps 500000 \
    --n-envs 8

# Alta performance
python scripts/train_pilot.py \
    --algorithm PPO \
    --n-envs 16 \
    --total-timesteps 1000000 \
    --learning-rate 3e-4 \
    --batch-size 64
```

#### Hiperpar√°metros Recomendados

```bash
--learning-rate 3e-4      # Default, funciona bien
--batch-size 64           # Equilibrado
--n-envs 8               # Standard (ajustar seg√∫n CPU)
--total-timesteps 500000  # M√≠nimo para convergencia
```

---

### SAC (Soft Actor-Critic)

**Recomendado para:** Circuitos complejos, convergencia r√°pida, exploraci√≥n

#### Caracter√≠sticas

| Aspecto | Valoraci√≥n | Detalles |
|---------|-----------|----------|
| **Estabilidad** | ‚≠ê‚≠ê‚≠ê | Generalmente estable, sensible a hiperpar√°metros |
| **Velocidad** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Muy r√°pida convergencia |
| **Facilidad de uso** | ‚≠ê‚≠ê‚≠ê | Requiere m√°s tuning que PPO |
| **Sample efficiency** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Excelente (off-policy con replay buffer) |
| **Exploraci√≥n** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Exploraci√≥n m√°xima (entropy regularization) |

#### Pros
- ‚úÖ **Convergencia r√°pida**: Suele aprender m√°s r√°pido que PPO
- ‚úÖ **Excelente exploraci√≥n**: Entropy bonus ayuda a descubrir estrategias
- ‚úÖ **Sample efficient**: Reutiliza experiencia pasada (replay buffer)
- ‚úÖ **Bueno para tareas complejas**: Maneja bien circuitos t√©cnicos
- ‚úÖ **Off-policy**: Puede aprender mientras explora

#### Cons
- ‚ö†Ô∏è **Sensible a hiperpar√°metros**: Requiere tuning cuidadoso
- ‚ö†Ô∏è **Mayor uso de memoria**: Replay buffer grande
- ‚ö†Ô∏è **Puede ser inestable**: Con mal tuning puede divergir
- ‚ö†Ô∏è **Requiere m√°s compute**: Buffer + double Q-networks

#### Cu√°ndo Usar SAC

- ‚úì Circuitos t√©cnicos y complejos
- ‚úì Quieres convergencia r√°pida
- ‚úì Tienes recursos computacionales
- ‚úì Necesitas buena exploraci√≥n
- ‚úì Est√°s dispuesto a tunear hiperpar√°metros

#### Ejemplo de Entrenamiento

```bash
# B√°sico
python scripts/train_pilot.py \
    --algorithm SAC \
    --total-timesteps 500000

# Circuito complejo
python scripts/train_pilot.py \
    --algorithm SAC \
    --track tracks/technical.json \
    --total-timesteps 500000 \
    --n-envs 16

# Alta performance
python scripts/train_pilot.py \
    --algorithm SAC \
    --n-envs 32 \
    --batch-size 256 \
    --learning-rate 3e-4 \
    --total-timesteps 1000000

# Multi-track
python scripts/train_pilot.py \
    --algorithm SAC \
    --multi-track \
    --n-envs 16 \
    --total-timesteps 1000000
```

#### Hiperpar√°metros Recomendados

```bash
--learning-rate 3e-4      # Standard para SAC
--batch-size 256          # M√°s grande que PPO
--n-envs 16              # Beneficia de m√°s paralelizaci√≥n
--total-timesteps 500000  # Suele converger m√°s r√°pido que PPO
```

---

### TD3 (Twin Delayed DDPG)

**Recomendado para:** Control preciso, time trials, comportamiento determin√≠stico

#### Caracter√≠sticas

| Aspecto | Valoraci√≥n | Detalles |
|---------|-----------|----------|
| **Estabilidad** | ‚≠ê‚≠ê‚≠ê‚≠ê | Estable, m√°s que SAC |
| **Velocidad** | ‚≠ê‚≠ê‚≠ê‚≠ê | Convergencia r√°pida |
| **Facilidad de uso** | ‚≠ê‚≠ê‚≠ê | Moderada, menos sensible que SAC |
| **Sample efficiency** | ‚≠ê‚≠ê‚≠ê‚≠ê | Muy buena (off-policy) |
| **Exploraci√≥n** | ‚≠ê‚≠ê‚≠ê | Exploraci√≥n moderada |

#### Pros
- ‚úÖ **Pol√≠tica determin√≠stica**: Comportamiento predecible
- ‚úÖ **Control preciso**: Excelente para maniobras finas
- ‚úÖ **M√°s estable que DDPG**: Twin critics reducen overestimation
- ‚úÖ **Off-policy**: Reutiliza experiencia pasada
- ‚úÖ **Buen equilibrio**: Entre estabilidad y velocidad

#### Cons
- ‚ö†Ô∏è **Menos exploraci√≥n que SAC**: Puede quedarse en √≥ptimos locales
- ‚ö†Ô∏è **Requiere replay buffer**: Mayor uso de memoria
- ‚ö†Ô∏è **Sensible a noise**: Necesita configurar noise adecuadamente

#### Cu√°ndo Usar TD3

- ‚úì Necesitas control determin√≠stico
- ‚úì Time trials / qualifying laps
- ‚úì Maniobras de precisi√≥n
- ‚úì Quieres algo entre PPO y SAC
- ‚úì Circuitos donde la precisi√≥n importa m√°s que la exploraci√≥n

#### Ejemplo de Entrenamiento

```bash
# B√°sico
python scripts/train_pilot.py \
    --algorithm TD3 \
    --total-timesteps 500000

# Time trial en circuito espec√≠fico
python scripts/train_pilot.py \
    --algorithm TD3 \
    --track tracks/monza.json \
    --total-timesteps 500000 \
    --n-envs 8

# Precision training
python scripts/train_pilot.py \
    --algorithm TD3 \
    --learning-rate 1e-3 \
    --batch-size 100 \
    --n-envs 8 \
    --total-timesteps 500000
```

#### Hiperpar√°metros Recomendados

```bash
--learning-rate 1e-3      # Puede ser m√°s alto que PPO/SAC
--batch-size 100          # Moderado
--n-envs 8               # Standard
--total-timesteps 500000  # Similar a SAC
```

---

## C√≥mo Entrenar

### Entrenamiento B√°sico

#### PPO (Recomendado para empezar)

```bash
python scripts/train_pilot.py \
    --algorithm PPO \
    --total-timesteps 500000
```

#### SAC (Para circuitos complejos)

```bash
python scripts/train_pilot.py \
    --algorithm SAC \
    --total-timesteps 500000 \
    --n-envs 16
```

#### TD3 (Para control preciso)

```bash
python scripts/train_pilot.py \
    --algorithm TD3 \
    --total-timesteps 500000
```

---

### Opciones de Entrenamiento

#### 1. Circuito Espec√≠fico

```bash
python scripts/train_pilot.py \
    --algorithm PPO \
    --track tracks/monza.json \
    --total-timesteps 500000
```

#### 2. Multi-Track (Generalizaci√≥n)

```bash
python scripts/train_pilot.py \
    --algorithm SAC \
    --multi-track \
    --total-timesteps 1000000 \
    --n-envs 16
```

#### 3. Por Dificultad

```bash
# Beginner
python scripts/train_pilot.py --difficulty 0 --total-timesteps 200000

# Intermediate
python scripts/train_pilot.py --difficulty 1 --total-timesteps 300000

# Advanced
python scripts/train_pilot.py --difficulty 2 --total-timesteps 500000

# Expert
python scripts/train_pilot.py --difficulty 3 --total-timesteps 1000000
```

#### 4. Curriculum Learning (Recomendado)

```bash
# Progresi√≥n autom√°tica de dificultad
python scripts/train_pilot.py \
    --curriculum \
    --algorithm PPO \
    --total-timesteps 1000000

# Empezar desde nivel intermedio
python scripts/train_pilot.py \
    --curriculum \
    --curriculum-level 1 \
    --total-timesteps 500000
```

üìñ **Ver:** [Curriculum Learning Guide](../docs/CURRICULUM_LEARNING.md)

#### 5. Transfer Learning

```bash
# Entrenar en √≥valo
python scripts/train_pilot.py \
    --track tracks/oval.json \
    --total-timesteps 200000 \
    --model-dir models/stage1/

# Transferir a circuito complejo
python scripts/train_pilot.py \
    --track tracks/monza.json \
    --load-model models/stage1/PPO_oval_final.zip \
    --total-timesteps 500000 \
    --model-dir models/stage2/
```

#### 6. Alta Performance

```bash
python scripts/train_pilot.py \
    --algorithm SAC \
    --n-envs 32 \
    --batch-size 256 \
    --total-timesteps 2000000 \
    --checkpoint-freq 100000
```

---

### Configuraci√≥n de Hiperpar√°metros

#### Learning Rate

```bash
# Conservador (m√°s estable, m√°s lento)
--learning-rate 1e-4

# Standard (recomendado)
--learning-rate 3e-4

# Agresivo (m√°s r√°pido, menos estable)
--learning-rate 1e-3
```

#### Batch Size

**PPO:**
```bash
--batch-size 64   # Standard
--batch-size 128  # M√°s estable
```

**SAC/TD3:**
```bash
--batch-size 256  # Recomendado
--batch-size 512  # Si hay memoria suficiente
```

#### Parallel Environments

```bash
--n-envs 4    # Recursos limitados
--n-envs 8    # Standard
--n-envs 16   # Alta performance
--n-envs 32   # M√°ximo (CPU potente)
```

**Nota:** M√°s entornos = m√°s datos/segundo pero m√°s CPU

#### Training Duration

```bash
--total-timesteps 100000     # Test r√°pido
--total-timesteps 500000     # Training standard
--total-timesteps 1000000    # Training completo
--total-timesteps 2000000    # Training extendido
```

---

## C√≥mo Evaluar

### Evaluaci√≥n B√°sica

Despu√©s de entrenar, eval√∫a el modelo:

```bash
python scripts/evaluate.py \
    --model trained_models/PPO_default_final.zip \
    --episodes 10
```

**Salida:**
- M√©tricas en consola
- JSON report: `results/PPO_default_evaluation.json`
- Plots: `results/PPO_default_plots.png`

### Evaluaci√≥n en Circuito Espec√≠fico

```bash
python scripts/evaluate.py \
    --model trained_models/PPO_monza_final.zip \
    --track tracks/monza.json \
    --episodes 20 \
    --output results/monza/
```

### Evaluaci√≥n con Visualizaci√≥n

```bash
# Render en tiempo real
python scripts/evaluate.py \
    --model trained_models/PPO_final.zip \
    --render \
    --episodes 5

# Grabar video
python scripts/evaluate.py \
    --model trained_models/PPO_final.zip \
    --record \
    --record-path recordings/best_lap.mp4
```

**Requiere:** `pip install opencv-python`

### Comparar Dos Modelos

```bash
python scripts/evaluate.py \
    --model trained_models/PPO_v1.zip \
    --compare trained_models/SAC_v1.zip \
    --episodes 20
```

**Salida:**
- Comparaci√≥n lado a lado
- Ganador por m√©trica
- Diferencias en %
- `results/comparison.json`

üìä **Ver:** [Evaluation Guide](../docs/EVALUATION_GUIDE.md)

---

## Comparaci√≥n de Todos los Modelos

### Script: Entrenar Todos los Algoritmos

```bash
#!/bin/bash
# train_all.sh

TRACK="tracks/monza.json"
TIMESTEPS=500000

# Entrenar con cada algoritmo
for algo in PPO SAC TD3; do
    echo "Entrenando $algo..."
    python scripts/train_pilot.py \
        --algorithm "$algo" \
        --track "$TRACK" \
        --total-timesteps "$TIMESTEPS" \
        --model-dir "models/${algo}/"
done

echo "Entrenamiento completo!"
```

**Ejecutar:**
```bash
chmod +x train_all.sh
./train_all.sh
```

---

### Script: Evaluar Todos los Modelos

```bash
#!/bin/bash
# evaluate_all.sh

TRACK="tracks/monza.json"
EPISODES=20

# Evaluar cada algoritmo
for algo in PPO SAC TD3; do
    echo "Evaluando $algo..."
    python scripts/evaluate.py \
        --model "models/${algo}/${algo}_monza_final.zip" \
        --track "$TRACK" \
        --episodes "$EPISODES" \
        --output "results/${algo}/"
done

echo "Evaluaci√≥n completa!"
echo "Resultados en: results/"
```

**Ejecutar:**
```bash
chmod +x evaluate_all.sh
./evaluate_all.sh
```

---

### Script: Comparaciones Directas

```bash
#!/bin/bash
# compare_algorithms.sh

TRACK="tracks/monza.json"
EPISODES=20

# PPO vs SAC
echo "Comparando PPO vs SAC..."
python scripts/evaluate.py \
    --model "models/PPO/PPO_monza_final.zip" \
    --compare "models/SAC/SAC_monza_final.zip" \
    --track "$TRACK" \
    --episodes "$EPISODES" \
    --output "results/comparison_PPO_vs_SAC/"

# SAC vs TD3
echo "Comparando SAC vs TD3..."
python scripts/evaluate.py \
    --model "models/SAC/SAC_monza_final.zip" \
    --compare "models/TD3/TD3_monza_final.zip" \
    --track "$TRACK" \
    --episodes "$EPISODES" \
    --output "results/comparison_SAC_vs_TD3/"

# PPO vs TD3
echo "Comparando PPO vs TD3..."
python scripts/evaluate.py \
    --model "models/PPO/PPO_monza_final.zip" \
    --compare "models/TD3/TD3_monza_final.zip" \
    --track "$TRACK" \
    --episodes "$EPISODES" \
    --output "results/comparison_PPO_vs_TD3/"

echo "Comparaciones completas!"
echo "Ver resultados en: results/comparison_*/"
```

---

### Python Script: Reporte Comparativo

```python
#!/usr/bin/env python3
"""
generate_comparison_report.py

Genera un reporte CSV comparando todos los modelos.
"""

import json
import csv
from pathlib import Path
from glob import glob

# Configuraci√≥n
results_dir = Path("results")
output_file = "algorithm_comparison.csv"

# Recopilar resultados de todos los algoritmos
all_results = []

for algo in ["PPO", "SAC", "TD3"]:
    json_file = results_dir / algo / f"{algo}_monza_evaluation.json"

    if json_file.exists():
        with open(json_file, 'r') as f:
            data = json.load(f)
            metrics = data['metrics']

            all_results.append({
                'Algorithm': algo,
                'Completion Rate': f"{metrics['completion_rate']:.2%}",
                'Best Lap Time': f"{metrics['lap_time_best']:.2f}s",
                'Mean Lap Time': f"{metrics['lap_time_mean']:.2f}s",
                'Lap Time Std': f"{metrics['lap_time_std']:.2f}s",
                'On Track %': f"{metrics['on_track_percentage']:.1f}%",
                'Off Track Count': int(metrics['off_track_count_total']),
                'Max Velocity': f"{metrics['max_velocity']:.1f} m/s",
                'Mean Reward': f"{metrics['total_reward_mean']:.2f}",
            })

# Guardar a CSV
with open(output_file, 'w', newline='') as f:
    if all_results:
        writer = csv.DictWriter(f, fieldnames=all_results[0].keys())
        writer.writeheader()
        writer.writerows(all_results)

print(f"‚úì Reporte guardado: {output_file}")
print(f"Total de algoritmos evaluados: {len(all_results)}")

# Imprimir tabla en consola
print("\n" + "="*80)
print("COMPARACI√ìN DE ALGORITMOS")
print("="*80)

for result in all_results:
    print(f"\n{result['Algorithm']}:")
    for key, value in result.items():
        if key != 'Algorithm':
            print(f"  {key:20s}: {value}")
```

**Ejecutar:**
```bash
python generate_comparison_report.py
```

---

### Evaluaci√≥n Multi-Track

Evaluar cada algoritmo en todos los circuitos:

```bash
#!/bin/bash
# evaluate_multitrack.sh

EPISODES=10
TRACKS=("tracks/oval.json" "tracks/simple.json" "tracks/technical.json")

for algo in PPO SAC TD3; do
    for track in "${TRACKS[@]}"; do
        track_name=$(basename "$track" .json)

        python scripts/evaluate.py \
            --model "models/${algo}/${algo}_multi_final.zip" \
            --track "$track" \
            --episodes "$EPISODES" \
            --output "results/${algo}/${track_name}/"
    done
done
```

---

## üìä Tabla Resumen: Qu√© Algoritmo Usar

| Escenario | Algoritmo Recomendado | Por Qu√© |
|-----------|----------------------|---------|
| Primera vez entrenando | **PPO** | M√°s estable y f√°cil |
| Circuito simple (√≥valo) | **PPO** | Suficiente y eficiente |
| Circuito t√©cnico complejo | **SAC** | Mejor exploraci√≥n |
| Quiero convergencia r√°pida | **SAC** | Off-policy, m√°s eficiente |
| Time trials / qualifying | **TD3** | Control determin√≠stico preciso |
| Recursos limitados | **PPO** | Menor uso de memoria |
| Multi-track generalizaci√≥n | **PPO** + curriculum | Estable en diferentes entornos |
| Exploraci√≥n importante | **SAC** | Entropy regularization |
| Necesito precisi√≥n | **TD3** | Pol√≠tica determin√≠stica |

---

## üí° Tips Finales

1. **Empieza con PPO** - Aprende el proceso con el algoritmo m√°s estable
2. **Usa curriculum learning** - `--curriculum` para mejor generalizaci√≥n
3. **Monitorea TensorBoard** - `tensorboard --logdir logs/`
4. **Guarda checkpoints** - Configurado por defecto cada 50k steps
5. **Eval√∫a frecuentemente** - Ver progreso en eval callback
6. **Compara algoritmos** - Prueba los 3 para ver cu√°l funciona mejor
7. **Ajusta n-envs** - Seg√∫n tu CPU (8-16 suele ser √≥ptimo)
8. **S√© paciente** - Buenas pol√≠ticas necesitan 500k-1M timesteps

---

## üìö Referencias

- [Evaluation Guide](../docs/EVALUATION_GUIDE.md) - C√≥mo evaluar modelos
- [Curriculum Learning](../docs/CURRICULUM_LEARNING.md) - Entrenamiento progresivo
- [Scripts README](README.md) - Documentaci√≥n principal
- [Main README](../README.md) - Visi√≥n general del proyecto
